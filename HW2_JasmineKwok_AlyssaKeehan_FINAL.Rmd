---
title: "Homework Assignment 2"
author: "Jasmine Kwok and Alyssa Keehan"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, echo=FALSE}
library(knitr)
library(tidyverse)
library(tree)
library(plyr)
library(class)
library(rpart)
library(maptree)
library(ROCR)
library(reshape2)

indent2 = '        '
```

### 1. Classification Task 
```{r, indent=indent2}
# read the dataset 
spam <- read_table2("spambase.tab", guess_max=2000)

# standardize each numerical attribute in the dataset.
# Each standardized column should have zero mean and unit variance.
spam <- spam %>%
mutate(y = factor(y, levels=c(0,1), labels=c("good", "spam"))) %>% # label as factors
mutate_at(.vars=vars(-y), .funs=scale) # scale others

# function that calculates misclassification error rate 
calc_error_rate <- function(predicted.value, true.value){
return(mean(true.value!=predicted.value))
}

# calculate the error rates to measure and compare classification performance
# keep track of error rates of all methods 
records = matrix(NA, nrow=3, ncol=2)
colnames(records) <- c("train.error","test.error")
rownames(records) <- c("knn","tree","logistic")

# split the data randomly into a test set and training set 
set.seed(1)
test.indices = sample(1:nrow(spam), 1000)
spam.train=spam[-test.indices,]
spam.test=spam[test.indices,]

# 10-fold cross validation 
nfold = 10
set.seed(1)
folds = seq.int(nrow(spam.train)) %>% ## sequential obs ids
cut(breaks = nfold, labels=FALSE) %>% ## sequential fold ids
sample ## random fold ids
```

#### K Nearest Neighbor Method 
Use 10-fold cross validation to select the best number of neighbors best.kfold out of six values of k in kvec = c(1, seq(10, 50, length.out=5)). Use the folds defined above and use the following do.chunk definition in your code. Again put set.seed(1) before your code. What value of k leads to the smallest estimated test error?
```{r, indent=indent2, cache=TRUE}
do.chunk <- function(chunkid, folddef, Xdat, Ydat, k){
  train = (folddef!=chunkid)
  Xtr = Xdat[train,]
  Ytr = Ydat[train]
  Xvl = Xdat[!train,]
  Yvl = Ydat[!train]
## get classifications for current training chunks
  predYtr = knn(train = Xtr, test = Xtr, cl = Ytr, k = k)
## get classifications for current test chunk
  predYvl = knn(train = Xtr, test = Xvl, cl = Ytr, k = k)
  data.frame(train.error = calc_error_rate(predYtr, Ytr),
           val.error = calc_error_rate(predYvl, Yvl))
}

# Specify XTrain, YTrain
Xtrain = spam.train %>% select(-y) %>% scale(center=TRUE, scale = TRUE)
Ytrain = spam.train$y
# Specify XTest, YTest
Xtest= spam.test %>% select(-y) %>% scale(center=TRUE, scale = TRUE)
Ytest = spam.test$y

# set error.folds as a vector to save future validation errors 
error.folds=NULL

# The possible number of nearest neighbors to be considered 
# a total of 6 values -  1 10 20 30 40 50 
kvec = c(1, seq(10, 50, length.out=5)) 

# set seed since do.chunk() contains random component induced by knn()
set.seed(1)

# loop through the different number of neighbors 
for (j in kvec){
  tmp = ldply(1:nfold, do.chunk, # apply do.chunk() to each fold
              folddef=folds, Xdat=Xtrain, Ydat=Ytrain, k=j)
  # the necessary arguments for do.chunk
  tmp$neighbors=j # keep track of each value of neighbors 
  error.folds= rbind(error.folds,tmp) #combine results 
}

# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('neighbors'), value.name='error')

# choose the number of neighbors which minimizes validation error 
val.error.means= errors %>% 
  filter(variable=='val.error') %>%
  group_by(neighbors,variable) %>%
  summarise_each(funs(mean),error)%>%
  ungroup()%>%
  filter(error==min(error))
val.error.means
```
```{r, indent=indent2, cache=TRUE}
# Best number of neighbors out of the 6 values 
best.kfold = max(val.error.means$neighbors)
best.kfold #10
```
_A k value of 10 leads to the smallest estimated test error._
### 2. Training and Test Errors 
Now that the best number of neighbors has been determined, compute the training error using spam.train and test error using spam.test or the k = best.kfold. Use the function calc_error_rate() to get the errors from the predicted class labels. Fill in the first row of records with the train and test error from the knn fit.
```{r, indent=indent2, cache=TRUE}
# compute training error and test error 
set.seed(1)
pred.Ytest = knn(train=Xtrain, test=Xtest, cl=Ytrain, k=best.kfold)
# error table for neighbors equal 10
errortable.10 <- error.folds %>% filter(neighbors==10)
# check if train error is the just average of train.error for all neighbors=10
train.error = mean(errortable.10$train.error)
test.error <- calc_error_rate(pred.Ytest, Ytest)
records[1,1] <- train.error 
records[1,2] <- test.error
records
```

### QUESTION 3 
(Controlling Decision Tree Construction) Function tree.control specifies options for tree construction:set minsize equal to 5 (the minimum number of observations in each leaf) and mindev equal to 1e-5. See the help for tree.control for more information. The output of tree.control should be passed into tree function in the control argument. Construct a decision tree using training set spam.train, call the resulting tree spamtree. summary(spamtree) gives some basic information about the tree. How many leaf nodes are there? How many of the training observations are misclassified?
```{r, indent=indent2,cache=TRUE}
spamtree <- tree(y ~ ., data = spam.train, 
                 control = tree.control(nobs = nrow(spam.train), minsize = 5, mindev = 1e-5))
summary(spamtree)
```
There are a total of 149 leaf nodes and there were 49 training observations that were misclassified out of the 3601 total predictions. 

### QUESTION 4
(Decision Tree Pruning) We can prune a tree using the prune.tree function. Pruning iteratively removes
the leaves that have the least effect on the overall misclassification. Prune the tree until there are only 10 leaf
nodes so that we can easily visualize the tree. Use draw.tree function from the maptree package to visualize
the pruned tree. Set nodeinfo=TRUE.
```{r, cache=TRUE}
# prune tree until there are only 10 leaf nodes
draw.tree(prune.tree(spamtree, best = 10), cex=0.5, nodeinfo = TRUE, col = NULL)
```

### QUESTION 5 
In this problem we will use cross validation to prune the tree. Fortunately, the tree package provides an easy to use function to do the cross validation for us with the cv.tree function. Use the same fold partitioning you used in the KNN problem (refer to cv.tree help page for detail about rand argument). Also be sure to set method=misclass. Plot the misclassification as function of tree size. Determine the optimal tree size that minimizes misclassification. Important: if there are multiple tree sizes that have the same minimum estimated misclassification, you should choose the smallest tree. This reflects the idea that we want to choose the simplest model that explains the data well (“Occam’s razor”). Show the optimal tree size best.size.cv in the plot.
```{r}
set.seed(1)
# K-fold cross validation 
cv.spamtree <- cv.tree(spamtree, rand=folds, FUN= prune.tree ,method="misclass") 
cv.spamtree
```
```{r, cache=TRUE}
# plotting misclassification function of tree size 
# "b" - for points and lines 
plot(cv.spamtree$size, cv.spamtree$dev/length(cv.spamtree),type = "b",
     xlab = "Tree Size", ylab = "CV Misclassification Rate")
best.size.cv = cv.spamtree$size[max(which(cv.spamtree$dev==min(cv.spamtree$dev)))]
abline(v=best.size.cv, lty=2)
```
```{r, indent=indent2, cache=TRUE}
# Best size 
best.size.cv #22
```
_The optimal tree size that minimizes missclassification is 22._

### QUESTION 6 
We previous pruned the tree to a small tree so that it could be easily visualized. Now, prune the original tree to 
size best.size.cv and call the new tree spamtree.pruned. Calculate the training error and test error when
spamtree.pruned is used for prediction. Use function calc_error_rate() to compute misclassification error. 
Also, fill in the second row of the matrix records with the training error rate and test error rate.
```{r, chache = TRUE}
# prune spamtree to size of best.size.cv
spamtree.pruned <- prune.misclass(spamtree, best=best.size.cv)

# Plot pruned tree
draw.tree(spamtree.pruned, cex=0.30, nodeinfo = TRUE, col=NULL)
plot(spamtree.pruned)
text(spamtree.pruned, pretty=0, col = "red", cex = 0.35)
title("Pruned tree of size 22")
```
```{r, indent=indent2, cache=TRUE}
# predict on test set 
pred.spamtree.pruned <- predict(spamtree.pruned, spam.test , type = "class")

# training error and test error using calc_error_rate()
train.error1 = mean(predict(spamtree.pruned, spam.train , type = "class")!=spam.train$y)
test.error1 <- calc_error_rate(pred.spamtree.pruned, Ytest)
records[2,1] <- train.error1
records[2,2] <- test.error1
records
```

### QUESTION 7
Logistic regression: 
a. Show that indeed the inverse of a logistic function is the logit function: 
$$
\begin{aligned}
p(z) &= {\frac{e^{z}}{1+e^{z}}}\\
let \hspace{0.2cm} p(z) &= y\\
y &= {\frac{e^{z}}{1+e^{z}}}\\
z &= {\frac{e^{y}}{1+e^{y}}}\\
z(1+e^{y}) &= e^{y}\\
z + ze^{y} &= e^{y}\\
z &= e^{y} - ze^{y}\\
z &= e^{y}(1-z)\\
{\frac{z}{1-z}} &= e^{y}\\
ln({\frac{z}{1-z}}) &= ln(e^{y})\\
ln({\frac{z}{1-z}}) &= y\\
therefore, \hspace{0.2cm} z(p) &= ln({\frac{p}{1-p}})
\end{aligned}
$$

b. The logit function is a commonly used link function for a generalized linear model of binary data. One reason
for this is that implies interpretable coefficients. Assume that $z = \beta_{0} + \beta_{1}x_{1}$, and p = logistic(z). How does the
odds of the outcome change if you increase $x_{1}$ by two? Assume $\beta_{1}$ is negative: what value does p approach as
$x_{1} \rightarrow \infty$? What value does p approach as $x_{1} \rightarrow -\infty$?

When we increase $x_{1}$ by two, this corresponds to a multiplicative change in the odds of $e^{2\beta_{1}}$. _

$$
\begin{aligned}
logit(p(z)) = \beta_{0} + \beta_{1}x_{1} + 2\beta_{1}\\
e^{logit(p(z)) }= e^{\beta_{0} + \beta_{1}x_{1} + 2\beta_{1}}\\
Odds(z) = e^{\beta_{0} + \beta_{1}x_{1} + 2\beta_{1}}\\
Odds(z) = e^{\beta_{0} + \beta_{1}x_{1}} \times e^{2\beta_{1}}\\
\end{aligned}
$$

Assuming $\beta_{1}$ is a negative value, as $x_{1} \rightarrow \infty$ the value of p becomes close to 0. 
$$
\begin{aligned}
p(z) &= {\frac{e^{\beta_{0} + \beta_{1}x_{1}}}{1+e^{\beta_{0} + \beta_{1}x_{1}}}}\\
since \hspace{0.2cm} \beta_{1} \hspace{0.2cm}is \hspace{0.2cm}a \hspace{0.2cm}negative \hspace{0.2cm}value, \hspace{0.2cm}
p(z) &= {\frac{e^{\beta_{0} - \beta_{1}x_{1}}}{1+e^{\beta_{0} - \beta_{1}x_{1}}}}\\
p(z) &= {\frac{e^{\beta_{0}}\times e^{-\beta_{1}x_{1}}}{1+e^{\beta_{0}} \times e^{-\beta_{1}x_{1}}}}\\
when \hspace{0.2cm} x_{1} \rightarrow \infty, \hspace{0.2cm} e^{-\beta_{1}x_{1}} \approx 0 \\
so \hspace{0.2cm} p(z) &= {\frac{e^{\beta_{0}}\times 0}{1+e^{\beta_{0}} \times 0 }}\\
therefore \hspace{0.2cm} p(z) \approx 0
\end{aligned}
$$


Assuming $\beta_{1}$ is a negative value, as $x_{1} \rightarrow -\infty$ the value of p becomes close to 1. 
$$
\begin{aligned}
p(z) &= {\frac{e^{\beta_{0} + \beta_{1}x_{1}}}{1+e^{\beta_{0} + \beta_{1}x_{1}}}}\\
since \hspace{0.2cm} \beta_{1} \hspace{0.2cm}is \hspace{0.2cm}a \hspace{0.2cm}negative \hspace{0.2cm}value, \hspace{0.2cm}
p(z) &= {\frac{e^{\beta_{0} - \beta_{1}x_{1}}}{1+e^{\beta_{0} - \beta_{1}x_{1}}}}\\
x_{1} \rightarrow -\infty \hspace{0.2cm} so, \hspace{0.2cm} p(z) &= {\frac{e^{\beta_{0} - (-\beta_{1}x_{1}})}{1+e^{\beta_{0} - (-\beta_{1}x_{1})}}}\\
and \hspace{0.2cm} p(z) &= {\frac{e^{\beta_{0} +\beta_{1}x_{1}}}{1+e^{\beta_{0} + \beta_{1}x_{1}}}}\\
p(z) &= {\frac{e^{\beta_{0}}\times e^{\beta_{1}x_{1}}}{1+e^{\beta_{0}} \times e^{\beta_{1}x_{1}}}}\\
when \hspace{0.2cm} x_{1} \rightarrow \infty, \hspace{0.2cm} e^{\beta_{1}x_{1}} \approx 9999999...(very\hspace{0.2cm} large\hspace{0.2cm} value) \\
so \hspace{0.2cm} p(z) &= {\frac{e^{\beta_{0}}\times 999999.....}{1+e^{\beta_{0}} \times 999999..... }}\\
therefore \hspace{0.2cm} p(z) \approx 1
\end{aligned}
$$


### QUESTION 8
Use logistic regression to perform classification. Logistic regression specifically estimates the probability that an
observation as a particular class label. We can define a probability threshold for assigning class labels based on
the probabilities returned by the glm fit.
In this problem, we will simply use the “majority rule”. If the probability is larger than 50% class as spam. Fit
a logistic regression to predict spam given all other features in the dataset using the glm function. Estimate the
class labels using the majority rule and calculate the training and test errors. Add the training and test errors
to the third row of records. Print the full records matrix. Which method had the lowest misclassification
error on the test set?
```{r, indent=indent2}
# create a model using glm 
glm_spamtree <- glm(y~.,data = spam ,family = "binomial")
#kable(summary(glm_spamtree)$coefficients, digits=3)
```
```{r, indent=indent2, cache=TRUE}
# fit training values 
fitted_val.train <- predict(glm_spamtree,spam.train, type="response")
#fitted_val.train
```
```{r, indent=indent2, cache=TRUE}
# confusion matrix 
err.glm.train <- table(Truth=spam.train$y,Prediction=ifelse(fitted_val.train>0.5,"spam", "good"))
#err.glm.train 
```
```{r, indent=indent2, cache=TRUE}
# predict on test set
fitted_val.test <- predict(glm_spamtree,spam.test, type="response")
# confusion matrix 
err.glm.test <- table(Truth=spam.test$y,Prediction=ifelse(fitted_val.test>0.5,"spam", "good"))
#err.glm.test 
```
```{r, indent=indent2, cache=TRUE}
# training error for glm model 
train.glm.err <-  1- sum(diag(err.glm.train))/sum(err.glm.train)
# testing error for glm model 
test.glm.err <-  1- sum(diag(err.glm.test))/sum(err.glm.test)

# saving the errors in the matrix
records[3,1] <- train.glm.err
records[3,2] <- test.glm.err 
records
```
The logistic method had the lowest misclassification error of 0.072 on the test set. 

### QUESTION 9
In the SPAM example, take “positive” to mean “spam”. If you are the designer of a spam filter, are you more
concerned about the potential for false positive rates that are too large or true positive rates that are too small?
Argue your case.

False positive rates in this case classifies an email that is not spam as spam while true positive rates classifies a spam email correctly. We are more concerned about the potential for large false positive rates as we might miss out on important emails that are misclassified. On the other hand, if the true positive rates are too small, we will just have to read more spam emails which does not have as much of a consequence. 
