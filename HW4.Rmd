---
title: "HW4"
author: "Alyssa Keehan"
date: "12/10/2020"
output: 
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tree)
library(ggplot2)
# install.packages('randomForest')
library(randomForest)
# install.packages('gbm')
library(gbm)
library(ROCR)
# install.packages('e1071')
library(e1071)
#install.packages('imager', dependencies = TRUE)
library(imager)
library(ISLR)
#install.packages("XQuartz")
#library("XQuartz")
```


## 1. Fundamentals of the Bootstrap

In the first part of this problem we will explore the fact that approximately 1/3 of the observations in a bootstrap
sample are out-of-bag.
a) Given a sample of size n, what is the probability that any observation j is not in in a bootstrap sample? Express
your answer as a function of n.
$$
\begin{aligned}
p(j\hspace{0.2cm}not\hspace{0.2cm}in\hspace{0.2cm}bootstrap\hspace{0.2cm}sample) &= (1-{\frac{1}{n}})^n\\
\end{aligned}
$$
b) Compute the above probability for n = 1000.
$$
\begin{aligned}
p(j\hspace{0.2cm}not\hspace{0.2cm}in\hspace{0.2cm}bootstrap\hspace{0.2cm}sample) &= (1-{\frac{1}{n}})^n\\
p(j\hspace{0.2cm}not\hspace{0.2cm}in\hspace{0.2cm}bootstrap\hspace{0.2cm}sample\hspace{0.2cm}|\hspace{0.2cm}n\hspace{0.2cm}=\hspace{0.2cm}1000 )&= (1-{\frac{1}{1000}})^{1000}\\
p(j\hspace{0.2cm}not\hspace{0.2cm}in\hspace{0.2cm}bootstrap\hspace{0.2cm}sample\hspace{0.2cm}|\hspace{0.2cm}n\hspace{0.2cm}=\hspace{0.2cm}1000 )&= (1-{\frac{1}{1000}})^{1000} \approx 0.3677\\
\end{aligned}
$$
c) Verify that your calculation is reasonable by resampling the numbers 1 to 1000 with replace and printing the
number of missing observations. Hint: use the unique and length functions to identify how many unique
observations are in the sample.
```{r}
set.seed(1)
use_samp <- sample(seq(1,1000), replace = TRUE)
num_unique <- length(unique(use_samp))
num_missing <- 1000 - num_unique
num_missing # divide by 1000, approximately equal to probability above
```
Here we’ll use the bootstrap to compute uncertainty about a parameter of interest.
d) By November 19, 2015, Stephen Curry, an NBA basketball player regarded as one of the best players currently
in the game, had made 62 out of 126 three point shot attempts (49.2%). His three point field goal percentage of
0.492, if he maintains it, will be one of the best all time for a single season. Use bootstrap resampling on a
sequence of 62 1’s (makes) and 64 0’s (misses). For each bootstrap sample compute and save the sample mean
(e.g. bootstrap FG% for the player). Use 1000 bootstrap samples to plot a histogram of those values. 
Compute
the 95% bootstrap confidence interval for Stephen Curry’s “true” end-of-season FG% using the quantile
function in R. Print the endpoints of this interval.
However, this estimate, and the associated uncertainty,
exclude information about his career performance as well as the typical shooting skill for other players in the
league. For reference, prior to this year, Stephen Curry had made about 43% of all three point shots in his
career. 
Despite the fact that the bootstrap histogram shows that it is about equally likely that Curry’s true
skill is greater or less than 0.492, why do you expect that his end-of-season field goal percentage will in fact be
lower than his percentage on 11/19? Hint: look up the phenomenon known as “regression to the mean”.
```{r}
set.seed(15)
curry_stat <- c(rep(1,62), rep(0,64))
curry_means <- c()
for (i in 1:1000){
  curry_sample <- sample(curry_stat, replace = TRUE)
  curry_means[i] <- mean(curry_sample)}
hist(curry_means)
```
```{r}
lower <- quantile(curry_means, 0.025)
upper <- quantile(curry_means, 0.975)
c(lower, upper)
```

The phenomenon regression towards the mean arises when a sample point--in this case, Stephen Curry's 3 point shot average-- is extremely high. The phenomenon states that a future point will be closer to the mean on futher measurements. Since the percentage was taken in November, towards the beginning of the season, it is hard to say that the average would stay consistent throughout the whole season, which usually lasts until spring of the next year. So although his 3 point percentage is really high at this point in the season, there is little possibility that it will stay at an extremely high rate the entire season. 


## 2. Eigenfaces

In this problem we will use PCA to explore variation in images of faces. Load the data saved in faces_array.RData
with the load function. This will load a 100 x 100 x 1000 array of data. An array is a generalization of a matrix to
more than 2 dimensions. In this example, the first two dimensions index the pixels in a 100 x 100 black and white
image of a face. The last dimension is the index for one of 1000 face images. The faces used in this example are from
1000 images scraped from the internet. See https://cyberextruder.com/face-matching-data-set-download/ for more
info.

```{r}
load("faces_array.RData")
```

Although it is natural to think about an stack of 1000 matrices representing each of the face images, to run PCA we
need to input a single matrix. To do this, we’ll convert each 100 x 100 matrix to a single vector of length 100*100 =
10000. When you call as.numeric on a matrix, it stacks each of the columns in the matrix into one large vector.
Thus, we can think of our data as 1000 observations of a 10000 variables (one variable per pixel). Run the following
code to get a matrix of face observations.

```{r}
face_mat <- sapply(1:1000, function(i) as.numeric(faces_array[, , i])) %>% t
```

When we want to visualization an image, we need to take the 10000 dimensional vector and reconstruct it as a matrix.
The code plot_face takes a single 10000 dimensional vector (e.g. a column of face_mat), converts it back to a
matrix, and plots the resulting image. You can test this functionality by printing a random face from the dataset:
plot_face(face_mat[sample(1000, 1), ]).

```{r}
plot_face <- function(image_vector) {
plot(as.cimg(t(matrix(image_vector, ncol=100))), axes=FALSE, asp=1)
}
plot_face(face_mat[sample(1000, 1), ])
```

a) Find the “average” face in this dataset by averaging all of the columns in face_mat. Plot the average face by
calling plot_face on the average.
```{r}
avg_face <- colMeans(face_mat)
plot_face(avg_face)
```
b) Run PCA on face_mat setting center=TRUE and scale=FALSE. In class we mentioned that in general it is best
if scale=TRUE because it puts all variables on the same scale and we don’t have to worry about the units of the
variables (remember, the scale of the variables affects our results). In general, this is good practice, especially
when the predictor variables are of mixed types. Here, each variable represents a single pixel intensity (in black
& white) and so all variables already have the same units and same scale (minimum of 0 and maximum of 255).
In this case, setting scale=FALSE actually seems to give slightly better results. Plot the PVE and cumulative
PVE from the PCA. How many PCs do you need to explain at least 50% of the total variation in the face
images?
```{r}
pr_face <- prcomp(face_mat, center = TRUE, scale = FALSE)
```
```{r}
pr_face.var <-  pr_face$sdev^2
pve_face <- pr_face.var/sum(pr_face.var)
plot(pve_face, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained",xlim = c(1,8), 
     ylim = c(0,1), type = "b")
plot(cumsum(pve_face), xlab = "Principal Component", 
     ylab = "Cummulative Proportion of Variance Explained",
     xlim = c(1,8), ylim = c(0,1), type = "b")

# used to compute the number of PCs to explain at least 50% of the variance
fifty_perc_var <- 1
for (i in c(1:length(pve_face))){
  if (cumsum(pve_face)[i] < 0.5){
    fifty_perc_var <- fifty_perc_var + 1
  }
  else{
    break
  }
}
fifty_perc_var
```
We need at least 5 PCs to explain at least 50% of the total variation in face images. 

c) Plot the first 16 principle component directions as faces using the plot_face function (these are the columns
of the rotation matrix). Early researchers termed these “eigenfaces” since they are eigenvectors of the
matrix of faces. The code below will adjust the margins of you plot and specifies a layout for the 16 images.
par(mfrow=c(4,4)) specifies a grid of 4 x 4 images. Each time you call plot_face it will plot the next face in
one of the new grid cells. All you need to do is call plot_face 16 times (please use a for loop). Note that these
images describe “directions” of maximum variability in the face images. You should interpret light and dark
regions in the eigenfaces as regions of high contrast, e.g. your interpretation should not change if you inverted
black and white in the images.

```{r}
par(mar = c(1,1,1,1))
par(mfrow=c(4,4))
for (i in 1:16){
  plot_face(pr_face$rotation[,i])
}
```

d) In this part, we will examine faces that have the highest and lowest values for specific PCs. Plot the faces
with the 5 largest values on PC1 and the 5 smallest values for PC1. Based on the example faces, and the first
eigenface from the previous part and the 10 example images, what aspect of variability in the face images is
captured by the first component.
```{r}
#pr_face$x[,1]
sorted_bottom_pc1 <- order(pr_face$x[,1])[1:5]
sorted_top_pc1 <- order(pr_face$x[,1],decreasing = TRUE)[1:5]
```
```{r}
par(mar = c(1,1,1,1))
par(mfrow=c(1,5))
for (i in sorted_top_pc1){
  plot_face(face_mat[i,])
}
```
```{r}
par(mar = c(1,1,1,1))
par(mfrow=c(1,5))
for (i in sorted_bottom_pc1){
  plot_face(face_mat[i,])
}
```

It looks like the variability captured by the first principal component is the darkness surrounding the face.
The higher PC1 values have a lighter background while the lower PC1 values have a darker background behind/
surrounding the face.

e) Repeat part d) but now display example faces with the largest and smallest values on principal component 5.
Again, discuss what aspect of variability in the face images is best captured by this principal component. Based
on your results, which principal component, (1 or 5) would be more useful as a feature in a face recognition
model (e.g. a model which predicts the identity of the individual in an image)
```{r}
sorted_bottom_pc5 <- order(pr_face$x[,5])[1:5]
sorted_top_pc5 <- order(pr_face$x[,5], decreasing = TRUE)[1:5]
```
```{r}
par(mar = c(1,1,1,1))
par(mfrow=c(1,5))
for (i in sorted_top_pc5){
  plot_face(face_mat[i,])
}
```
```{r}
par(mar = c(1,1,1,1))
par(mfrow=c(1,5))
for (i in sorted_bottom_pc5){
  plot_face(face_mat[i,])
}
```

The aspect of variability that is best captured in the 5th principal component is the length/type of the hair on the person's head. It looks like with a lower PC5 value, they person has less hair and with a higher PC5, the person has more/longer hair. I believe PC5 would be better at identifying a person's face because the hair is good indicator of a person's identity. Since PC1 only looks at the background behind the person's face, it is not as strong of an identifier as someone's hair/ hair length.

## 3. Logistic Regression with Polynomial Features

a) In class, we have used polynomial linear regression several times as an example for model complexity and the
bias variance tradeoff. We can also introduce polynomial logistic regression models to derive more sophisticated
classification functions by introducing additional features. Use read_csv to load nonlinear.csv and plot the
data. Plot each point colored according to its class, Y.
```{r}
non_linear <- read_csv("nonlinear.csv")
```
```{r}
ggplot(non_linear, aes(x = X1, y = X2, color = Y)) + geom_point()
```
b) Fit a logistic regression model of Y on X1 and X2. The decision boundary can be visualized by making predictions
of class labels over finely sampled grid points that cover your region (sample space) of interest. The following
code will create grid points over the sample space as below:

```{r}
# grid of points over sample space
gr <- expand.grid(X1=seq(-5, 5, by=0.1), # sample points in X1
X2=seq(-5, 5, by=0.1)) # sample points in X2
```

For each point in gr, predict a class label using the logistic regression model. You should classify based on
the probability being greater or less than 1/2. Visualize your predictions at each point on the grid using the
geom_raster function. This function colors in rectangles on the defined grid and is a good way to visualize
your decision boundary. Set the fill aesthetic to your predicted label and outside of the aes use alpha=0.5 to
set the transparency of your predictions. Plot the observed data, colored by label, over the predictions using
geom_point.
```{r}
non_lin_glm <- glm(Y ~ X1 + X2, data = non_linear, family = binomial)
summary(non_lin_glm)
pred_gr <- predict(non_lin_glm, gr, type = "response")
new_pred_gr <- c()
for (i in c(1:range(length(pred_gr)))){
  if (pred_gr[i] > 0.5){
    new_pred_gr[i] <- "High"
  }
  else{
    new_pred_gr[i] <- "Low"
  }
}
#new_pred_gr

# how can I use both 
nonlin_raster <- ggplot(gr, aes(X1, X2), alpha = 0.5) + 
  geom_raster(aes(fill = new_pred_gr)) + 
  geom_point(data = non_linear,aes(col=Y, size=0.00015)) +
  geom_point(data = non_linear, colour="yellow")
nonlin_raster

```
c) Fit a model involving 2nd degree polynomial of X1 and X2 with interaction terms. You should use the poly()
function. Inspect result of the fit using summary(). Plot the resulting decision boundary.
```{r}
poly_nonlin <- glm(Y~poly(X1, 2)*poly(X2,2),data = non_linear, family = binomial)
summary(poly_nonlin)
```
```{r}
pred_poly <- predict(poly_nonlin, gr, type = "response")
new_pred_poly <- c()
for (i in c(1:range(length(pred_poly)))){
  if (pred_poly[i] > 0.5){
    new_pred_poly[i] <- "High"
  }
  else{
    new_pred_poly[i] <- "Low"
  }
}
poly_raster <- ggplot(gr, aes(X1, X2), alpha = 0.5) + 
  geom_raster(aes(fill = new_pred_poly)) + 
  geom_point(data = non_linear,aes(col=Y, size=0.00015)) +
  geom_point(data = non_linear, colour="orange")
poly_raster
```
d) Using the same procedure, fit a logistic regression model with 5-th degree polynomials without any interaction
terms. Inspect result of the fit using summary(). Plot the resulting decision boundary and discuss the result.
Explain the reason for any strange behavior.
```{r}
poly5_nonlin <- glm(Y~poly(X1, 5) + poly(X2, 5),data = non_linear, family = binomial)
summary(poly5_nonlin)
```
```{r}
pred_poly5 <- predict(poly5_nonlin, gr, type = "response")
new_pred_poly5 <- c()
for (i in c(1:range(length(pred_poly5)))){
  if (pred_poly5[i] > 0.5){
    new_pred_poly5[i] <- "High"
  }
  else{
    new_pred_poly5[i] <- "Low"
  }
}
poly5_raster <- ggplot(gr, aes(X1, X2), alpha = 0.5) + 
  geom_raster(aes(fill = new_pred_poly5)) + 
  geom_point(data = non_linear,aes(col=Y, size=0.00015)) +
  geom_point(data = non_linear, colour="purple")
poly5_raster
```

The strange result can be highlighted in the teal area surrounding most of the left border and the entire top border of the graph. This is strange because none of the points are located in that section, let alone any points classifying Y as 0. All of the points with that classification are in a cluster towards the right center of the graph so it is just odd that the top left border/corner is covered in an area representing a low prediction value. You can say that the blue region representing a low classification is overfitting. The reason behind this strange behavior might be because a larger p will result in higher variance and a lower bias. Just like when we were first learning about what makes a model have high variance and low bias, the features of this graph show that is is non-parametric because it is also nonlinear. 

e)Qualitatively, compare the relative magnitudes of coefficients of in the two polynomial models and the linear
model. What do you notice? Your answer should mention bias, variance and/or overfitting.

The overall magnitudes of the coefficients in the polynomial models are higher than those of the linear model. Typically when we are dealing with polynomial regression, a larger p means higher variance and lower bias. This is present when we look at the graphs of the corresponding polynomial models. As you can see, the 5th degree polynomial model's decision boundary is overfitting in the top left hand corner and sides of the graph despite not having any points in that region. This is because of the high variance that comes with a larger degree value. In addition to a large p value and high variance comes higher values of coefficients that we see in both polynomial models' summaries. Putting together the characteristics the polynomial model--high variance, low bias, nonlinear, complex, flexible--, we can conclude that they are both non parametric. In contrast, since the linear model's graph shows lower coefficients, lower variance, higher bias, and inflexibility, we can conclude that it is a parametric model.

f) (231 required, 131 extra credit) Create 3 bootstrap replicates of the original dataset. Fit the linear model
and the 5th order polynomial to each of the bootstrap replicates. Plot class predictions on the grid of values
for each of both linear and 5th order fits, from each of the bootstrap samples. There should be six plots total.
Discuss what you see in the context of your answer to the previous question. 

```{r}
set.seed(1)
bootstrap1ind <- sample(c(1:72), replace = TRUE)
bootstrap2ind <- sample(c(1:72), replace = TRUE)
bootstrap3ind <- sample(c(1:72), replace = TRUE)
bootstrap1 <- non_linear[bootstrap1ind,]
bootstrap2 <- non_linear[bootstrap2ind,]
bootstrap3 <- non_linear[bootstrap3ind,]
```
```{r}
# linear model with first bootstrap
non_lin_glm1 <- glm(Y ~ X1 + X2, data = bootstrap1, family = binomial)
summary(non_lin_glm1)
pred_gr1 <- predict(non_lin_glm1, gr, type = "response")
new_pred_gr1 <- c()
for (i in c(1:range(length(pred_gr1)))){
  if (pred_gr1[i] > 0.5){
    new_pred_gr1[i] <- "High"
  }
  else{
    new_pred_gr1[i] <- "Low"
  }
}

# plotting linear model with first bootstrap
nonlin_raster1 <- ggplot(gr, aes(X1, X2), alpha = 0.5) + 
  geom_raster(aes(fill = new_pred_gr1)) + 
  geom_point(data = bootstrap1,aes(col=Y, size=0.00015)) +
  geom_point(data = bootstrap1, colour="red") +
  ggtitle("Linear Model of the First Bootstrap")
```
```{r}

# 5th degree polynomial with the first model
poly5_nonlin1 <- glm(Y~poly(X1, 5) + poly(X2, 5),data = bootstrap1, family = binomial)
summary(poly5_nonlin1)
pred_poly51 <- predict(poly5_nonlin1, gr, type = "response")
new_pred_poly51 <- c()
for (i in c(1:range(length(pred_poly51)))){
  if (pred_poly51[i] > 0.5){
    new_pred_poly51[i] <- "High"
  }
  else{
    new_pred_poly51[i] <- "Low"
  }
}
poly5_raster1 <- ggplot(gr, aes(X1, X2), alpha = 0.5) + 
  geom_raster(aes(fill = new_pred_poly51)) + 
  geom_point(data = bootstrap1,aes(col=Y, size=0.00015)) +
  geom_point(data = bootstrap1, colour="red") +
  ggtitle("5th Degree Polynomial Model of the First Bootstrap")
# plot both below
nonlin_raster1
poly5_raster1
```
```{r}
# linear model with second bootstrap
non_lin_glm2 <- glm(Y ~ X1 + X2, data = bootstrap2, family = binomial)
summary(non_lin_glm2)
pred_gr2 <- predict(non_lin_glm2, gr, type = "response")
new_pred_gr2 <- c()
for (i in c(1:range(length(pred_gr2)))){
  if (pred_gr2[i] > 0.5){
    new_pred_gr2[i] <- "High"
  }
  else{
    new_pred_gr2[i] <- "Low"
  }
}

# plotting linear model with second bootstrap
nonlin_raster2 <- ggplot(gr, aes(X1, X2), alpha = 0.5) + 
  geom_raster(aes(fill = new_pred_gr2)) + 
  geom_point(data = bootstrap2,aes(col=Y, size=0.00015)) +
  geom_point(data = bootstrap2, colour="tan") +
  ggtitle("Linear Model of the Second Bootstrap")
```
```{r}
# 5th degree polynomial with the second model
poly5_nonlin2 <- glm(Y~poly(X1, 5) + poly(X2, 5),data = bootstrap2, family = binomial)
summary(poly5_nonlin2)
pred_poly52 <- predict(poly5_nonlin2, gr, type = "response")
new_pred_poly52 <- c()
for (i in c(1:range(length(pred_poly52)))){
  if (pred_poly52[i] > 0.5){
    new_pred_poly52[i] <- "High"
  }
  else{
    new_pred_poly52[i] <- "Low"
  }
}
poly5_raster2 <- ggplot(gr, aes(X1, X2), alpha = 0.5) + 
  geom_raster(aes(fill = new_pred_poly52)) + 
  geom_point(data = bootstrap2,aes(col=Y, size=0.00015)) +
  geom_point(data = bootstrap2, colour="tan") +
  ggtitle("5th Degree Polynomial Model of the Second Bootstrap")
# plot both below
nonlin_raster2
poly5_raster2
```
```{r}
# linear model with third bootstrap
non_lin_glm3 <- glm(Y ~ X1 + X2, data = bootstrap3, family = binomial)
summary(non_lin_glm3)
pred_gr3 <- predict(non_lin_glm3, gr, type = "response")
new_pred_gr3 <- c()
for (i in c(1:range(length(pred_gr3)))){
  if (pred_gr3[i] > 0.5){
    new_pred_gr3[i] <- "High"
  }
  else{
    new_pred_gr3[i] <- "Low"
  }
}

# plotting linear model with third bootstrap
nonlin_raster3 <- ggplot(gr, aes(X1, X2), alpha = 0.5) + 
  geom_raster(aes(fill = new_pred_gr3)) + 
  geom_point(data = bootstrap3,aes(col=Y, size=0.00015)) +
  geom_point(data = bootstrap3, colour="white") +
  ggtitle("Linear Model of the Third Bootstrap")
```
```{r}
# 5th degree polynomial with the third model
poly5_nonlin3 <- glm(Y~poly(X1, 5) + poly(X2, 5),data = bootstrap3, family = binomial)
summary(poly5_nonlin3)
pred_poly53 <- predict(poly5_nonlin3, gr, type = "response")
new_pred_poly53 <- c()
for (i in c(1:range(length(pred_poly53)))){
  if (pred_poly53[i] > 0.5){
    new_pred_poly53[i] <- "High"
  }
  else{
    new_pred_poly53[i] <- "Low"
  }
}
poly5_raster3 <- ggplot(gr, aes(X1, X2), alpha = 0.5) + 
  geom_raster(aes(fill = new_pred_poly53)) + 
  geom_point(data = bootstrap3,aes(col=Y, size=0.00015)) +
  geom_point(data = bootstrap3, colour="white") +
  ggtitle("5th Degree Polynomial Model of the Third Bootstrap")
# plot both below
nonlin_raster3
poly5_raster3
```

Similar to my result in part d of this problem, there is a huge difference between the significantly higher magnitudes for the coefficients in the 5th degree polynomial models and the smaller magnitudes of for the coefficients in the linear models. I noticed that for the linear plots, the model is very inflexible, it has high bias which leads to oversmoothing and they have a low variance. In contrast, the 5th degree polynomial model shows complexity, high variability by overfitting and low bias. Since a large p means large variance and a large variance causes higher coefficient values it is easy for a high degree polynomial model can overfit. Looking at the models themselves, the first bootstrap model is the least flexible because the blue region barely covers any of the points with their respective classification. This is evident with its relatively lower 
coefficient values seen in the first linear bootstrap model's summary. Looking at the 5th degree polynomial bootstrap models, it looks like the third model is the most flexible. this is based off by looking at the relatively higher coefficient values and the decision boundary plotted above. Besides the center teal region, the two other teal clusters located on the left border and slightly above the main teal oval are signs that it is overfitting the most. In both of those regions, there are no points classified as 0, it is just empty, unoccupied space. 

## 4. Predicting Insurance Policy Purchases

This question involves the use of the “Caravan” data set, which contains 5822 real customer records. Each record
consists of 86 variables, containing sociodemographic data (variables 1-43) and product ownership (variables 44-86),
grouped by zip code. In this problem we will focus on predicted the variable “Purchase” which indicates whether
the customer purchased a caravan insurance policy. For more information see http://www.liacs.nl/~putten/library/
cc2000/data.html.

a) When you load the “ISLR” library, the variable Caravan is automatically loaded into your environment. Split
Carvan into a training set consisting of the first 1000 observations and a test set consisting of the remaining
observations.
```{r}
caravan_train <- Caravan[1:1000,]
caravan_test <- Caravan[1001:5822,]
```
b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors.
Use the gbm to fit a 1,000 tree boosted model and set the shrinkage value of 0.01. Which predictors appear to
be the most important (Hint: use the summary function)?
```{r}
set.seed(2)
car_boost <- gbm(ifelse(Purchase == "Yes", 1,0)~., data = caravan_train, 
                 distribution = "bernoulli", n.trees = 1000, shrinkage = 0.01)
summary(car_boost)
```

The top 10 predictors that appear to be the most important are PPERSAUT, MKOOPKLA, MOPLHOOG, MBERMIDD, ABRAND, MINK3045, MOSTYPE, PBRAND, MGODGE and MGODOV. 

c) Now fit a random forest model to the same training set from the previous problem. Set importance=TRUE but
use the default parameter values for all other inputs to the randomForest function. Print the random forest
object returned by the random forest function. 
What is the out-of-bag estimate of error? How many variables
were subsampled at each split in the trees? How many trees were used to fit the data? Look at the variable
importance. Is the order of important variables similar for both boosting and random forest models?
```{r}
set.seed(3)
rf_caravan <- randomForest(Purchase ~., data = caravan_train, importance = TRUE)
print(rf_caravan)
importance(rf_caravan)
varImpPlot(rf_caravan, n.var = 5)
```

The out of bag estimate error is 6.1%. 
9 samples were subsampled at each split in the tree.
500 trees were used to fit the data.
Based on the importance values and the importance plots above,
the most important variables are MRELOV, MOPLMIDD, APLEZIER, MAUT0, MBERMIDD, MOPLHOOG, 
MOSTYPE, MGODGE, PPERSAUT and MGODPR. The order of importance is not really similar because 
the top variable in the Boosted model is PPERSAUT and it below in importance to MOSTYPE and 
MGODGE in the random forest model. Each model only contains a few that are important in 
both models; in this case, those variables are MBERMIDD, MOPLHOOG, MOSTYPE, MGODGE and PPERSAUT.

d) Use both models to predict the response on the test data. Predict that a person will make a purchase if the
estimated probability of purchase is greater than 20 %. Print the confusion matrix for both the boosting and
random forest models. In the random forest model, what fraction of the people predicted to make a purchase do
in fact make one? Note: use the predict function with type="prob" for random forests and type="response"
for the boosting algorithm.

```{r}
set.seed(6)
# predict on the testing data: boosting
car_test_boost <- predict(car_boost, newdata = caravan_test, type = "response")

new_car_test_boost <- c()
for (i in c(1:4822)){
  if (car_test_boost[i] > 0.2){
    new_car_test_boost[i] <- "Yes"
  }
  else{
    new_car_test_boost[i] <- "No"
  }
}
# confustion matrix
boost_error <- table(pred = new_car_test_boost, truth = caravan_test$Purchase)
boost_error
test_boost_error <- 1 - sum(diag(boost_error))/sum(boost_error)
test_boost_error # 0.0786
```

```{r}
set.seed(9)
# predict on the testing data: random forest
car_test_rf <- predict(rf_caravan, newdata = caravan_test, type = "prob")
yes_col <- car_test_rf[,2]
new_rf_pred <- c()
for (i in c(1:4822)){
  if (yes_col[i] > 0.2){
    new_rf_pred[i] <- "Yes"
  }
  else{
    new_rf_pred[i] <- "No"
  }
}
# confusion matrix
rf_error <- table(pred = new_rf_pred, truth = caravan_test$Purchase)
rf_error
test_rf_error <- 1 - sum(diag(rf_error))/sum(rf_error)
test_rf_error # 0.1047
```
In the random forest model, only 46 of the people predicted to make a purchase actually made a purchase, making the fraction 46/308 = 0.149 (46 people correctly predicted yes over the total number of predicted yes people). 

## 5. SVMs prediction of drug use

```{r}
drug_use <- read_csv('drug.csv',
col_names = c('ID','Age','Gender','Education','Country','Ethnicity',
'Nscore','Escore','Oscore','Ascore','Cscore','Impulsive',
'SS','Alcohol','Amphet','Amyl','Benzos','Caff','Cannabis',
'Choc','Coke','Crack','Ecstasy','Heroin','Ketamine','Legalh','LSD',
'Meth', 'Mushrooms', 'Nicotine', 'Semer','VSA'))
```

a) Split the data into training and test data. Use a random sample of 1500 observations for the training data and
the rest as test data. Use a support vector machine to predict recent_cannabis_use using only the subset of
predictors between Age and SS variables as on homework 3. Unlike homework 3, do not bother mutating the
features into factors. Use a “radial” kernel and a cost of 1. Generate and print the confusion matrix of the
predictions against the test data.
```{r}
drug_use <- drug_use %>% mutate(recent_cannabis_use =
factor(ifelse(Cannabis >= "CL3", "Yes", "No"),
levels=c("No", "Yes")))
drug_use_subset <- drug_use %>% select(Age:SS, recent_cannabis_use)
```
```{r}
# randomly splitting
set.seed(10)
drug_samp <- sample(1:nrow(drug_use_subset), 1500)
drug_train <- drug_use_subset[drug_samp,]
drug_test <- drug_use_subset[-drug_samp,]

drug_svm <- svm(recent_cannabis_use~. ,data = drug_train, kernel = "radial", cost = 1)
summary(drug_svm)
```
```{r}
drug_pred <- predict(drug_svm, drug_test)
table(predict = drug_pred, truth = drug_test$recent_cannabis_use)
```

b) Use the tune function to perform cross validation over the set of cost parameters: cost=c(0.001, 0.01, 0.1,
1,10,100). What is the optimal cost and corresponding cross validated training error for this model? Print the
confusion matrix for the best model. The best model can be found in the best.model variable returned by
tune.
```{r}
set.seed(8)
drug_tuned <- tune(svm, recent_cannabis_use ~., data = drug_train, kernel = "radial", 
                   ranges = list(cost=c(0.001, 0.01, 0.1, 1,10,100)))
summary(drug_tuned)
```
```{r}
bestmod <- drug_tuned$best.model # 0.1
summary(bestmod)
```
```{r}
# use the test data
tuned_pred <- predict(bestmod, drug_test)
table(predict = tuned_pred, truth = drug_test$recent_cannabis_use)
```

The optimal cost is 0.1 and the corresponding cv training error is 0.1913333 



