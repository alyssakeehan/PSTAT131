---
title: "Homework 1"
author: "Hera Chan and Alyssa Keehan"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document: 
    latex_engine: xelatex
  html_document:
    df_print: paged
---



```{r setup, echo=FALSE}
library(knitr)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5)
options(digits = 4)

library(readr)
library(tinytex)
library(hflights)
library(tidyverse)
library(dplyr)
library(class)
library(tidyverse)
library(ISLR)
library(ggplot2)
library(reshape2)
library(plyr)
library(dplyr)
library(class)
algae <- read_table2("algae_data/algaeBloom.txt", col_names= c('season','size','speed','mxPH','mnO2','Cl','NO3','NH4', 'oPO4','PO4','Chla','a1','a2','a3','a4','a5','a6','a7'),
 na="XXXXXXX")
glimpse(algae)
```


# 1. Descriptive summary statistics 
Given the lack of further information on the problem domain, it is wise to investigate some of the statistical properties of the data, so as to get a better grasp of the problem. It is always a good idea to start our analysis with some kind of exploratory data analysis. A first idea of the statistical properties of the data can be obtained through a summary of its descriptive statistics.

(a) Count the number of observations in each season using summarise() in dplyr.
```{r}
algae %>%
  dplyr::group_by(season) %>%
  dplyr::summarise(n=n(), na.rm = TRUE)
```
_According to the data presented above, the number of observations taken in Autumn are 40, Spring are 53, Summer are 43 and Winter are 62._

(b) Are there missing values? Calculate the mean and variance of each chemical (Ignore a1 through a7). What do you notice about the magnitude of the two quantities for different chemicals?
```{r}
#checking for missing values
# is.na(algae) > returns a lot of values, some of them being TRUE
chem_tibb <- algae %>% dplyr::select(mxPH:Chla)
chem_mean <- chem_tibb%>%
  summarise_all(mean, na.rm =TRUE) 
chem_var <- chem_tibb%>%
  summarise_all(var, na.rm =TRUE) 
chem_sum1 <- rbind(chem_mean, chem_var)
chem_sum1
```

_Yes there are missing values, I applied the na.rm = True to fix my N/A problem. I noticed that the larger the mean, the higher the magnitude (difference) between the mean and the variance. The only two data sets where the mean is greater than the variance are max PH value and minimum Oxygen level. This might be because they are both averages of max/min values instead of an average of average values. When comparing the other 6 with each other, the smallest magnitude between the mean and variance occurs between NO3, which has the lowest mean. Looking at the NH4 mean and variance, the mean is the biggest out of all the other columns and the magnitude between the mean and variance is the greatest._ 

c. Mean and Variance is one measure of central tendency and spread of data. Median and Median Absolute Deviation are alternative measures of central tendency and spread. For a univariate data set X1,X2, ...,Xn, the Median Absolute Deviation (MAD) is defined as the median of the absolute deviations from the data’s median:
            MAD = median(|Xi − median(X)|)
Compute median and MAD of each chemical and compare the two sets of quantities (i.e., mean & variance vs. median & MAD). What do you notice?
```{r}
chem_med <- chem_tibb%>%
  summarise_all(median, na.rm = TRUE)
chem_mad <- chem_tibb%>%
  summarise_all(mad, na.rm =TRUE) 
```
```{r}
chem_summ <- rbind(chem_mean, chem_var, chem_med, chem_mad)
rownames(chem_summ) <- c("Mean", "Variance", "Median", "MAD")
chem_summ
"not sure why this isn't working to name the rows but
1: mean
2: variance
3: median
4: mad "
```

_The majority of values for median and MAD are all less than the values for mean and variance of the 8 columns. Looking at the 4 right columns, the magnitude between the median and MAD is signficantly less than the magnitude between the mean and the variance. For the two values on the left (average of max PH values and average of min O2 levels), the magnitude is slightly higher for the median and MAD than for the mean and variance._ 

# 2. Data visualization 
Most of the time, the information in the data set is also well captured graphically. Histogram, scatter plot, boxplot, Q-Q plot are frequently used tools for data visualization. Use ggplot for all of these visualizations.

(a) Produce a histogram of mxPH with the title ‘Histogram of mxPH’ based on algae data set. Use an appropriate argument to show the probability instead of the frequency as the vertical axis. (Hint: look at the examples in the help file for function geom_histogram()). Is the distribution skewed?
```{r}
algae_mxPH <- algae %>% ggplot(aes(x = mxPH)) +
  geom_histogram(mapping = aes(y = ..density..), binwidth = 0.1, na.rm = TRUE, fill = 'dark green') 
mxPH_hist <- algae_mxPH + ggtitle("Histogram of mxPH")
mxPH_hist
```

_The distribution doesn't look skewed at all. It looks like a majority of values for amount of mxPH circle around about 8 for a maximum ph value._ 

(b) Add a density curve using geom_density() and rug plots using geom_rug() to above histogram.
```{r}
mxPH_hitplusdens <- mxPH_hist +
  geom_density(mapping = aes(x = mxPH, y = ..density..), na.rm = TRUE, color = "blue") +
  geom_rug(sides="b")
mxPH_hitplusdens
```

(c) Create a boxplot with the title ‘A conditioned Boxplot of Algal a1’ for a1 grouped by size. (Refer to help page for geom_boxplot()).

```{r}
ggplot(data = algae) + 
  geom_boxplot(mapping = aes(x = size, y = a1, fill = size), na.rm = TRUE)
```

(d) Are there any outliers for NO3 and NH4? How many observations would you consider as outliers? How did you arrive at this conclusion?
```{r}
no3_box <- ggplot(data = algae) + 
  geom_boxplot(mapping = aes(x = size, y = NO3, fill = size), na.rm = TRUE)
nh4_box <- ggplot(data = algae) +
  geom_boxplot(mapping = aes(x = size, y = NH4, fill = size), na.rm = TRUE)
no3_box
nh4_box
```

_When looking at the NO3 data, I see one outlier in the data for a medium sized body of water and about 4 outliers for the data in small bodies of water. Looking at the NH4 data, the data from large bodies of water have 3 outliers, the data from a medium sized body of water has about 8 outliers, with one being a really far outlier at a value of almost 25,000, and the data from a small body of waterhaving about 7 outliers. In the boxplot, if data is represented by a point, it typically means they are outliers._ 

(e) Compare mean & variance vs. median & MAD for NO3 and NH4. What do you notice? Can you conclude which set of measures is more robust when outliers are present?

```{r}
NO3_summ <- c(mean(algae$NO3, na.rm = TRUE), var(algae$NO3, na.rm =TRUE),
              median(algae$NO3, na.rm = TRUE), mad(algae$NO3, na.rm = TRUE))
NH4_summ <- c(mean(algae$NH4, na.rm = TRUE), var(algae$NH4, na.rm =TRUE),
              median(algae$NH4, na.rm = TRUE), mad(algae$NH4, na.rm = TRUE))

sum_col <- c("Mean", "Variance", "Median","MAD" )
sum_row <- c("NO3", "NH4")

chem_summary <- rbind(NO3_summ, NH4_summ)
colnames(chem_summary) <- sum_col
rownames(chem_summary) <- sum_row
chem_summary
```

_Based off of what we can see in the summary statistics for both chemicals, NO3 is has significantly less values in all statistics. Although there are some outliers, as shown in the boxplots for 2d, the variance for NO3 is significantly less than the variance for NH4. The boxplot for NH4 data shows a very small area for the 25% - 75% quantile because the outlier is very far away from the median/mean and the scale is significantly bigger than the scale for NO3. The outlier in the NH4 data creates a more robust value for variance in the summary statistics. Since the variance takes into consideration the the mean of all values, it will be more skewed left than the mad which only looks at medians._ 

## Predicting Algae Blooms
Some water samples contained unknown values in several chemicals. Missing data are very common in
real-world problems, and may prevent the use of certain data mining techniques that are not able to handle missing values.

In this homework, we are going to introduce various ways to deal with missing values. After all the missing values have been taken care of, we will build a model to investigate the relationship between the variable a1 and other 11 predictors (season, size, speed, mxPH, mnO2, Cl, NO3, NH4, oPO4, PO4, Chla) utilizing cross-validation in the next problem.
# 3. Dealing with missing values
(a) How many observations contain missing values? How many missing values are there in each variable?
```{r}
algae %>%
  dplyr::select(season:a1) %>%  # replace to your needs
  summarise_all(funs(sum(is.na(.))))
```

Based on my output above, mxPH, mnO2, Cl, NO3, NH4, oPO4, PO4 and CHla all have missing values. The columns and their missing values are as follows:
mxPh: 1 , mnO2: 2, Cl: 10, NO3: 2, NH4: 2, oPO4: 2, PO4: 2 and Chla: 12. There are a total of 16 observations with missing data 

(b) Removing observations with missing values: use filter() function in dplyr package to observations with any missing value, and save the resulting dataset (without missing values) as algae.del. Report how many observations are in algae.del. Hint: complete.cases() may be useful.
```{r}
algae.del <- algae%>%
  dplyr :: select(season:a1)%>%
  filter(complete.cases(.))
algae.del
```

_There are a total of 184 observations in the new dataset "algae.del". _

(c) Imputing unknowns with measures of central tendency: the simplest and fastest way of filling in (imputing) missing values is to use some measures of central tendency such as mean, median and mode. 

Use mutate_at() and ifelse() in dplyr to fill in missing values for each chemical with its median, and save the imputed dataset as algae.med. Report the number of observations in algae.med. Display the values of each chemical for the 48th, 62th and 199th obsevation in algae.med. This simple strategy, although extremely fast and thus appealing for large datasets, imputed values may have large bias that can influence our model fitting. An alternative for decreasing bias of imputed values is to use relationships between variables.
```{r}
algae.med <- algae %>% 
  dplyr::select(season:a1)%>%
  mutate_at(vars(starts_with("m")|starts_with("C") | starts_with('N') | starts_with('o') |
                   starts_with("P")), funs(ifelse(is.na(.),median(., na.rm = TRUE),.)))
spec_algae.med <- algae.med %>%
  slice(which(row_number() == 48 | row_number() == 62 | row_number() == 199))
spec_algae.med
```
(d) Imputing unknowns using correlations: another way to impute missing values is to use correlation with another variable. For a highly correlated pair of variables, we can fill in the unknown values by predicting one based on the other with a simple linear regression model, provided the two variables are not both unknown. 
Compute pairwise correlation between the continuous (chemical) variables.
Then, fill in the missing value for PO4 based on oPO4 in the 28th observation. What is the value
you obtain? Hint: use lm() and predict() function.

```{r}
algae_use <- algae.del%>%
  dplyr::select(mxPH:Chla)
cor(algae_use)
```
We see a strong correlation between oPO4 and PO4.

```{r}
po4_opo4.lm <- lm(algae$PO4 ~ algae$oPO4)
summary(po4_opo4.lm)
```
```{r}
algae%>%
  dplyr::select(season:a1)%>%
  slice(which(row_number()==28))
```
_The value of oPO4 at the 28th observation is 4. _

```{r}
ci <- predict(po4_opo4.lm, data.frame(g=4), interval = "confidence", level = 0.95, 
             type = "response")
ci[28] #need to do this or get a value at every observation, only need 28th
```
_Based on the value of oPO4 being 4, we can predict that the mean value of PO4 is about 48.07._

```{r}
algae$PO4[28] <- 48.07
algae%>%
  dplyr::select(season:a1)%>%
  slice(which(row_number()==28))
```

(e) Questioning missing data assumptions: When might imputation using only the observed data lead you to incorrect conclusions? In a couple of sentences, describe a scenario in which the imputed values of the chemical abundances in the algae data (imputed using either the median or correlation method) might be a poor substitute for the true missing values. Hint: look at the example from lecture 2.

_Imputations using only the observed data can lead us to incorrect conclusions by giving us the wrong idea of the actual presence of certain chemicals in rivers. Looking at the lists of specific chemical they are testing with, a lot of them are very dangerous and toxic to humans. There is a possibility that maybe, while collecting data samples, the scietists were exposed to a lot of the chemicals and they were deemed unable to continue with the experiment. The lack of data might have to do with the process affecting the scientists themselves. This is an example of survivorship bias because the lack of data questions the actual presence of the chemical in the test site. In this case, where the chemicals were too much for the scientists to handle, it is not wise to replace that value with the median since it is obviously an above average case. Similarly, we cannot assume that the chemicals are linearly related to other chemicals just based off of the observed data._  

##Estimating the Test Error with Cross Validation (CV)
# 4. Cross-validation: 
In class we talked about how to use cross-validation (CV) to estimate the “test error”. In k-fold CV, each of k equally sized random partitions of data (chunks) are used in a heldout set (called validation set or test set). After k runs, we average the held-out error as our final estimate of the validation error. For this part, we will run cross-validation on only a single model, as a way to estimate our test error for future predictions (we are not using it here for model selection since we are considering only one model). Perform 5-fold cross-validation on this model to estimate the (average) test error.

Using algae.med dataset obtained in (3c), we will build a linear regression model to predict the levels of algae type a1 based on 11 variables (season, size, speed, mxPH, mnO2, Cl, NO3, NH4, oPO4, PO4, Chla), and test generalization of model to data that have not been used for training.

(a) First randomly partition data into 5 equal sized chunks.
Hint: a simple way to randomly assign each observation to a chunk is to do the following. First, use cut(..., label=FALSE) to divide observation ids (1, 2, . . . ) into equal numbers of chunk ids. Then, randomize output of cut()by using sample().

**Answer**
```{r}
set.seed(561)

ob_id <- (1:200)
chonk <- cut(ob_id, breaks = 5, label = FALSE)
chonkdef <- sample(chonk, 200, replace = TRUE)
chonkdef
```
(b) Perform 5-fold cross-validation with training error and validation errors of each chunk determined from (4a). Since same computation is repeated 5 times, we can define the following function for simplicity.

```{r}
do.chunk <- function(chunkid, chunkdef, dat){ # function argument
  train = (chunkdef != chunkid)
  
  Xtr = dat[train,1:11] # get training set
  Ytr = dat[train,12] # get true response values in trainig set

  Xvl = dat[!train,1:11] # get validation set
  Yvl = dat[!train,12] # get true response values in validation set

  lm.a1 <- lm(a1~., data = dat[train,1:12])
  predYtr = predict(lm.a1) # predict training values
  predYvl = predict(lm.a1,Xvl) # predict validation values
  
  data.frame(fold = chunkid,
      train.error = mean((predYtr - Ytr$a1)^2), # compute and store training error
      val.error = mean((predYvl - Yvl$a1)^2)) # compute and store test error
}
```

First argument chunkid indicates which chunk to use as validation set (one of 1:5). Second argument chunkdef is chunk assignments from (4a). Third argument dat will be algae.med dataset. 
In order to repeatedly call do.chunk() for each value of chunkid, use functions lapply() or ldply(). Note that chunkdef and dat should be passed in as optional arguments (refer to help pages).
Write the code and print out the train.error and val.error five times (e.g. for each chunk).
**Answer**
```{r}
set.seed(679)
errors <- ldply(1:5, do.chunk, chunkdef = chonkdef, dat = algae.med)
errors
```
_Now, I will take the average of the validation error to get an estimate of what I expect the error to be on a new dataset_
```{r}
errors%>%
  dplyr::select(val.error)%>%
  summarise_all(mean)
```

5. Test error on additional data: now imagine that you actually get new data that wasn’t available when you first fit the model.

(a) Additional data can be found in the file algaeTest.txt.
```{r}
algae.Test <- read_table2('algae_data/algaeTest.txt',
                    col_names=c('season','size','speed','mxPH','mnO2','Cl','NO3',
                                'NH4','oPO4','PO4','Chla','a1'),
                    na=c('XXXXXXX'))
```

This data was not used to train the model and was not (e.g. wasn’t used in the CV procedure to estimate the test error). We can get a more accurate measure of true test error by evaluating the model fit on this held out set of data. Using the same linear regression model from part 4 (fit to all of the training data), calculate the “true” test error of your predictions based on the newly collected measurements in algaeTest.txt. Is this roughly what you expected based on the CV estimated test error from part 4?

**Answer**
#supposed to use predict function
```{r}
# using the model we got from question 4
lm.a1 <- lm(a1~., data = algae.Test)

# use the predictions from the Test file
algae.predictions <- predict(lm.a1, algae.Test)

TrueTestError <- mean((algae.predictions-algae.Test$a1)^2)
TrueTestError
```

_Yes, this is about what we expected for the result based on the estimated test error from question 4. The average validation error from cross validation of the original set was larger than the error we determined from the new set. This is expected because the new dataset will give us a more accurate depiction of the response to the new model, which is why the error is in the same range but a little lower._


# Cross Validation (CV) for Model Selection
In this problem, we will be exploring a dataset of wages from a group of 3000 workers. The goal in this part is to identify a relationship between wages and age.

6. First, install the ISLR package, which includes many of the datasets used in the ISLR textbook. Look at the variables defined in the Wage dataset. We will be using the wage and age variables for this problem.

```{r}
library(ISLR)
head(Wage)
```

(a) Plot wages as a function of age using ggplot. Your plot should include the datapoints (geom_point()) as well as a smooth fit to the data (geom_smooth()). Based on your visualization, what is the general pattern of wages as a function of age? Does this match what you expect?
```{r}
ggplot(data = Wage, mapping = aes(x = age, y = wage)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

Based on our graph, the data points and smooth fit line show a parabolic relationship between age and wage. It reaches its peak at between the ages of 40 and 60 and the lowest occurs at the beginning (20 years of age) and the end (80 year of age). This matches what I expect because typically, when you first start working, you will not make as much as a senior employee. The longer you work, the more you will typically get paid. The downfall is also represented by the age of retirement. We see a downfall at around 60 year old which is around the typical age of retirement and it goes down even more til it reaches 80 years old. 

(b) In this part of the problem, we will find a polynomial function of age that best fits the wage data. For each polynomial function between p = 0, 1, 2, ...10:
          i. Fit a linear regression to predict wages as a function of age, age2, . . . agep (you 
            should include an intercept as well). Note that p = 0 model is an “intercept-only” model.
          ii. Use 5-fold cross validation to estimate the test error for this model. Save both the 
            test error and the training error.

i.
```{r}

folds.mod <- cut(1:nrow(Wage), breaks = 5, labels= FALSE)%>%sample()

do.chonk <- function(chunkid, chunkdef, dat,p){ # function argument
  train = (chunkdef != chunkid)
  
  Xtr = dat[train,] # get training set
  Ytr = dat[train,] # get true response values in trainig set

  Xvl = dat[!train,] # get validation set
  Yvl = dat[!train,] # get true response values in validation set
  
  if(p==0)
  {
    lm.mod = lm(wage~1, data = dat[train,])
  }
  else
  {
    lm.mod = lm(wage~poly(age, degree = p, raw = FALSE), data = dat[train,])
  }

  predYtr = predict(lm.mod) # predict training values
  predYvl = predict(lm.mod,Xvl) # predict validation values
  
  data.frame(fold = chunkid,
      train.error = mean((predYtr - Ytr$wage)^2), # compute and store training error
      val.error = mean((predYvl - Yvl$wage)^2)) # compute and store test error
}
```

ii.
```{r}
errors.mod = NULL
set.seed(834)
for(k in 0:10)
{
  temp.mod <- ldply(1:5, do.chonk, chunkdef = folds.mod, dat = Wage, p = k)
  temp.mod$degree<-k
  errors.mod <- rbind(errors.mod, temp.mod)
}
errors.mod
```

```{r}
errors.mod%>%
  dplyr::select(train.error:val.error)%>%
  summarise_all(mean)
```
_Our average training error is 1611 and our average test error is 1616._

(c) Plot both the test error and training error (on the same plot) for each of the models estimated above as a function of p. What do you observe about the training error as p increases? What about the test error? Based on your results, which model should you select and why?


```{r dplyr}
"grouping errors by degrees"
use_new_error_sum <- errors.mod%>%
  group_by(degree)%>%
  summarise_at(vars(train.error, val.error), list(name = mean))
use_new_error_sum
```


```{r}
# Plot errors
ggplot() + 
  geom_line(data = use_new_error_sum, aes(x = degree, y = val.error_name, color = "red")) + 
  geom_line(data = use_new_error_sum, aes(x = degree, y = train.error_name, color = "blue")) +
  labs(title = "degrees vs. error", x="degrees", y="error", color = "Type of Errors") +
  scale_color_discrete(name = "Error Term", labels = c("Training", "Validation"))
"Not sure why my code is assigning a different color to the values but the graph 
is correct in terms of values
i.e. Validation error ends up beinging higher than training error"
```

_Based off of what I observe from the graph, as p increases, the training error decreases. Similaryly, the test error also reacts in the same way: as p increases, the error decreases. When p increases, the error decreases at a slower rate and sort of flattens out at p = 2. As a result, our training error ends up being less than the validation error. Based on our results, We would choose the model with the highest degrees (in this case p = 10) with 2 folds. This is because the validation error at this point is the smallest in the whole model._

Note: poly(age, degree=p, raw=TRUE) will return a matrix with p columns, where the p-th column is agep. For the predictors in your regression use poly(age, degree=p, raw=FALSE). The raw=FALSE option returns predictors which are numerically more stable (it returns a matrix of “orthogonal polynomials”). Numerical stability can be an issue because agep can be very very large if p is large. The orthogonal polynomials returned when raw=FALSE are rescaled to account for this so please use the raw=FALSE option. Hint: A function similar to do.chunk from problem 4 will be helpful here as well.


